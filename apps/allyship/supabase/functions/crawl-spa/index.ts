/// <reference lib="deno.ns" />
import { createClient } from 'jsr:@supabase/supabase-js@2'
import puppeteer from 'npm:puppeteer@^21.0.0'
import { normalizeUrlForCrawling } from 'npm:@allystudio/url-utils@^1.0.0'

interface CrawlStats {
  total: number
  new: number
  existing: number
  skipped: number
  errors: number
}

interface CrawlResult {
  urls: string[]
  stats: CrawlStats
}

// Extract links from page using Puppeteer evaluation
async function extractLinksFromPage(
  page: puppeteer.Page,
  baseUrl: string
): Promise<string[]> {
  // Wait for any dynamic content to load
  await page.waitForTimeout(2000)

  // Extract all links including those generated by JavaScript
  const links = await page.evaluate(baseUrl => {
    const anchors = document.querySelectorAll('a[href]')
    const linkSet = new Set<string>()

    // Get all anchor tag links
    anchors.forEach(anchor => {
      const href = anchor.getAttribute('href')
      if (href) {
        try {
          const absoluteUrl = new URL(href, window.location.href).href
          linkSet.add(absoluteUrl)
        } catch (e) {
          // Invalid URL, skip
        }
      }
    })

    // Also look for router-link elements (Vue.js)
    const routerLinks = document.querySelectorAll('[to], [href]')
    routerLinks.forEach(link => {
      const to = link.getAttribute('to') || link.getAttribute('href')
      if (to && to.startsWith('/')) {
        try {
          const absoluteUrl = new URL(to, window.location.href).href
          linkSet.add(absoluteUrl)
        } catch (e) {
          // Invalid URL, skip
        }
      }
    })

    // Look for data attributes that might contain routes
    const dataLinks = document.querySelectorAll(
      '[data-href], [data-to], [data-route]'
    )
    dataLinks.forEach(link => {
      const dataHref =
        link.getAttribute('data-href') ||
        link.getAttribute('data-to') ||
        link.getAttribute('data-route')
      if (dataHref && dataHref.startsWith('/')) {
        try {
          const absoluteUrl = new URL(dataHref, window.location.href).href
          linkSet.add(absoluteUrl)
        } catch (e) {
          // Invalid URL, skip
        }
      }
    })

    return Array.from(linkSet)
  }, baseUrl)

  return links
}

// Check if URL should be crawled
function shouldCrawlUrl(url: string, baseUrl: string): boolean {
  try {
    const urlObj = new URL(url)
    const baseObj = new URL(baseUrl)

    // Must be same domain
    if (urlObj.origin !== baseObj.origin) return false

    // Skip file extensions
    if (
      url.match(
        /\.(jpg|jpeg|png|gif|pdf|doc|docx|zip|rar|exe|dmg|json|xml|csv|txt|ico|svg|js|css|woff|woff2|ttf|eot)$/i
      )
    )
      return false

    // Skip mailto, tel, javascript
    if (
      url.startsWith('mailto:') ||
      url.startsWith('tel:') ||
      url.startsWith('javascript:')
    )
      return false

    // Skip fragments
    if (url.includes('#')) return false

    return true
  } catch (error) {
    return false
  }
}

// Main crawling function using Puppeteer
async function crawlSPA(
  browser: puppeteer.Browser,
  startUrl: string,
  maxPages: number = 100,
  maxDepth: number = 3
): Promise<CrawlResult> {
  const stats: CrawlStats = {
    total: 0,
    new: 0,
    existing: 0,
    skipped: 0,
    errors: 0,
  }
  const discoveredUrls = new Set<string>()
  const crawledUrls = new Set<string>()
  const urlQueue: Array<{ url: string; depth: number }> = [
    { url: startUrl, depth: 0 },
  ]

  console.log(`[SPA-CRAWL] Starting SPA crawl of ${startUrl}`)

  // Create a new page for crawling
  const page = await browser.newPage()

  try {
    // Set viewport and user agent
    await page.setViewport({ width: 1440, height: 900, deviceScaleFactor: 1 })
    await page.setUserAgent(
      'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    )

    // Set longer timeout for SPAs
    page.setDefaultTimeout(60000)

    while (urlQueue.length > 0 && discoveredUrls.size < maxPages) {
      const { url: currentUrl, depth } = urlQueue.shift()!

      // Skip if already crawled or depth exceeded
      if (crawledUrls.has(currentUrl) || depth > maxDepth) {
        stats.skipped++
        continue
      }

      console.log(`[SPA-CRAWL] Crawling ${currentUrl} at depth ${depth}`)
      crawledUrls.add(currentUrl)

      try {
        // Navigate to the page
        await page.goto(currentUrl, {
          waitUntil: 'networkidle0', // Wait for network to be idle (important for SPAs)
          timeout: 60000,
        })

        // Additional wait for JavaScript to render content
        await page.waitForTimeout(3000)

        // Check if page loaded successfully
        const title = await page.title()
        console.log(`[SPA-CRAWL] Page loaded: ${title}`)

        // Add current URL to discovered URLs
        const normalized = normalizeUrlForCrawling(currentUrl, startUrl)
        if (normalized) {
          if (!discoveredUrls.has(normalized.url)) {
            discoveredUrls.add(normalized.url)
            stats.new++
            stats.total++
            console.log(`[SPA-CRAWL] New URL discovered: ${normalized.url}`)
          } else {
            stats.existing++
          }
        }

        // Extract links from the page
        const links = await extractLinksFromPage(page, startUrl)
        console.log(`[SPA-CRAWL] Found ${links.length} links on ${currentUrl}`)

        // Process each link
        for (const link of links) {
          if (shouldCrawlUrl(link, startUrl) && !crawledUrls.has(link)) {
            const normalized = normalizeUrlForCrawling(link, startUrl)
            if (normalized && !discoveredUrls.has(normalized.url)) {
              urlQueue.push({ url: normalized.url, depth: depth + 1 })
              console.log(`[SPA-CRAWL] Queued for crawling: ${normalized.url}`)
            }
          }
        }

        // Small delay between requests to be respectful
        await page.waitForTimeout(1000)
      } catch (error) {
        console.error(`[SPA-CRAWL] Error crawling ${currentUrl}:`, error)
        stats.errors++
      }
    }
  } finally {
    await page.close()
  }

  console.log(`[SPA-CRAWL] Crawl completed. Stats:`, stats)
  return {
    urls: Array.from(discoveredUrls),
    stats,
  }
}

// Main Deno serve handler
Deno.serve(async (req: Request) => {
  let browser: puppeteer.Browser | undefined

  try {
    const { url, website_id, max_pages = 100, max_depth = 3 } = await req.json()

    if (!url) {
      return new Response(
        JSON.stringify({
          success: false,
          error: { message: 'URL parameter is required' },
        }),
        { headers: { 'Content-Type': 'application/json' }, status: 400 }
      )
    }

    // Validate environment variables
    const browsercatApiKey = Deno.env.get('BROWSERCAT_API_KEY')
    const supabaseUrl = Deno.env.get('NEXT_PUBLIC_SUPABASE_URL')
    const serviceRoleKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')

    if (!browsercatApiKey) throw new Error('BROWSERCAT_API_KEY is not set')
    if (!supabaseUrl) throw new Error('NEXT_PUBLIC_SUPABASE_URL is not set')
    if (!serviceRoleKey) throw new Error('SUPABASE_SERVICE_ROLE_KEY is not set')

    console.log(`[SPA-CRAWL] Starting SPA crawl of ${url}`)

    // Connect to browsercat
    console.log('[BROWSER] Connecting to browsercat')
    browser = await puppeteer.connect({
      browserWSEndpoint: `wss://api.browsercat.com/connect?apiKey=${browsercatApiKey}`,
    })
    console.log('[BROWSER] Connected successfully')

    // Perform the SPA crawl
    const result = await crawlSPA(browser, url, max_pages, max_depth)

    // If website_id is provided, create pages in database
    if (website_id && result.urls.length > 0) {
      const supabase = createClient(supabaseUrl, serviceRoleKey)

      console.log(
        `[DATABASE] Creating ${result.urls.length} pages for website ${website_id}`
      )

      // Prepare page data
      const pageData = result.urls.map(pageUrl => {
        const normalized = normalizeUrlForCrawling(pageUrl, url)
        return {
          website_id,
          url: normalized?.path || '/',
          full_url: pageUrl,
          status: 'discovered',
          discovered_at: new Date().toISOString(),
        }
      })

      // Insert pages (ignore duplicates)
      const { data: insertedPages, error: insertError } = await supabase
        .from('Page')
        .upsert(pageData, {
          onConflict: 'website_id,url',
          ignoreDuplicates: true,
        })
        .select()

      if (insertError) {
        console.error('[DATABASE] Error inserting pages:', insertError)
      } else {
        console.log(
          `[DATABASE] Successfully created/updated ${insertedPages?.length || 0} pages`
        )
      }
    }

    return new Response(
      JSON.stringify({
        success: true,
        data: {
          urls: result.urls,
          stats: result.stats,
          message: `Successfully crawled SPA and discovered ${result.urls.length} pages`,
        },
      }),
      { headers: { 'Content-Type': 'application/json' } }
    )
  } catch (error) {
    console.error('[SPA-CRAWL] Error:', error)
    return new Response(
      JSON.stringify({
        success: false,
        error: {
          message:
            error instanceof Error ? error.message : 'Unknown error occurred',
          type: 'spa_crawl_error',
        },
      }),
      {
        headers: { 'Content-Type': 'application/json' },
        status: 500,
      }
    )
  } finally {
    if (browser) {
      console.log('[BROWSER] Disconnecting from remote browser')
      try {
        await browser.disconnect()
      } catch (error) {
        console.warn('[BROWSER] Failed to disconnect properly:', error)
      }
    }
  }
})
